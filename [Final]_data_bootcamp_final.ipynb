{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0TSBOBWlE4QL"
      },
      "source": [
        "# **Data Bootcamp Final Project**\n",
        "## By Elizabeth Tang\n",
        "\n",
        "---\n",
        "### **Project Description**: Holiday-Driven Consumer Insight Analysis of Engagement on Social Media Across Brands"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wmAxcF3tbdNs"
      },
      "source": [
        "*   Does consumer sentiments increase or decrease around holidays for different brands?\n",
        "\n",
        "*   How does social media engagement correlate with holiday-specific advertising expenditures?\n",
        "\n",
        "*   Can we classify whether a day is “holiday period” vs “normal period” through sentiment and engagement analysis?\n",
        "\n",
        "*   Is it possible to cluster brands into groups based on holiday sentiment patterns?\n",
        "\n",
        "*   How to forecast brand perception around holidays using time-series models?\n",
        "\n",
        "*   Can neural networks predict sentiment numeric WRDS metrics (ANN regression), classification of holiday vs nonholiday periods (ANN classification), and textual news headlines from RavenPack (ANN NLP model)?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9bWM3eZ-sL7Y"
      },
      "source": [
        "### **Purpose**\n",
        "\n",
        "This project analyzes how consumer sentiment, engagement, and behavior toward major brands shift around holidays and seasonal events (such as Black Friday, Christmas, Super Bowl, or Valentine's Day) by using a combination of social media data, advertising expenditure, and market performance from WRDS datasets (RavenPack) and Google Trends.\n",
        "\n",
        "The final project integrates econometrics, supervised and unsupervised machine learning, time-series forecasting, and modern neural networks in order to visualize holiday-driven shifts in consumption, predicting consumer engagement, grouping brands by holiday season performance, and forecasts brand reputation over time, as well as analyzes news headlines/social buzz with NLP neural networks.\n",
        "\n",
        "### **Contents**\n",
        "1. Dataset: RavenPack\n",
        "2. Dataset: Google Trends\n",
        "3. Dataset: US Holiday Calendar\n",
        "4. Dataset: Merged Data\n",
        "5. Analysis Questions\n",
        "6. Conclusion"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Cnetp8gJsO3x"
      },
      "source": [
        "\n",
        "\n",
        "---\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lfTQzTPzEjGc"
      },
      "outputs": [],
      "source": [
        "# # install WRDS (needed once)\n",
        "# !pip install wrds\n",
        "\n",
        "# # install pytrends for google trends\n",
        "# !pip install pytrends\n",
        "# !pip install wrds pytrends pandas numpy scikit-learn statsmodels tensorflow keras xgboost matplotlib seaborn plotly holidays prophet\n",
        "\n",
        "# # install prophet (needed once) --> used for time series forecasting\n",
        "# !pip install cmdstanpy prophet\n",
        "\n",
        "# # install transformers\n",
        "# !pip install transformers\n",
        "\n",
        "# # install Streamlit\n",
        "# !pip install streamlit\n",
        "\n",
        "import streamlit as st\n",
        "from transformers import pipeline\n",
        "\n",
        "import os, sys, time\n",
        "from datetime import timedelta, datetime\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import holidays\n",
        "\n",
        "# modeling\n",
        "import statsmodels.api as sm\n",
        "import statsmodels.formula.api as smf\n",
        "from sklearn.model_selection import train_test_split, TimeSeriesSplit, GridSearchCV\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.neighbors import NearestNeighbors\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.cluster import KMeans\n",
        "from sklearn.metrics import accuracy_score, classification_report, mean_squared_error, confusion_matrix\n",
        "from sklearn.neural_network import MLPRegressor, MLPClassifier\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense, Dropout"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import wrds\n",
        "database = wrds.Connection()\n",
        "\n",
        "# see the libraries available to my WRDS account\n",
        "database.list_libraries()"
      ],
      "metadata": {
        "id": "IqDys8BVc_sM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from pytrends.request import TrendReq\n",
        "\n",
        "# US location\n",
        "pytrends = TrendReq(hl = 'en-US', tz = 360)"
      ],
      "metadata": {
        "id": "RjWB_26ZFWCJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hkLj46YPaciw"
      },
      "outputs": [],
      "source": [
        "# clone data from github files from Elizabeth (LIZonga)\n",
        "# !git clone -b final_data_files https://github.com/LIZonga/data_bootcamp_final.git"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qMecHtinh2CA"
      },
      "source": [
        "# **Dataset: RavenPack**\n",
        "\n",
        "Includes information on news, social media, files, and insight like sentiment scores and impact. This data can help with optimization.\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# SQL for managing and manipulating data in relational databases\n",
        "sql = \"\"\"\n",
        "SELECT\n",
        "    CAST(rpa_date_utc AS DATE) AS date,\n",
        "    entity_name AS brand,\n",
        "    AVG(event_sentiment_score) AS sentiment,\n",
        "    SUM(rp_story_event_count) AS mentions,\n",
        "    SUM(relevance) AS buzz\n",
        "FROM ravenpack_trial.rpa_full_equities\n",
        "WHERE rpa_date_utc BETWEEN '2020-01-01' AND '2025-01-01'\n",
        "GROUP BY date, brand\n",
        "ORDER BY date, brand\n",
        "\"\"\"\n",
        "# create the ravenpack DataFrame\n",
        "ravenpack_df = database.raw_sql(sql)"
      ],
      "metadata": {
        "id": "W5cUqve8SBTa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fc5bbf2a"
      },
      "source": [
        "ravenpack_df.info()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "e96fd648"
      },
      "source": [
        "# show the DataFrame\n",
        "ravenpack_df.head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# clean up the data\n",
        "ravenpack_df['date'] = pd.to_datetime(ravenpack_df['date'])\n",
        "# fill NaN sentiment values with 0 (neutral sentiment) instead of dropping rows\n",
        "ravenpack_df['sentiment'] = ravenpack_df['sentiment'].fillna(0)\n",
        "\n",
        "# show the DataFrame\n",
        "ravenpack_df.head()"
      ],
      "metadata": {
        "id": "kyNT863yYXUP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Dataset: Google Trends**\n",
        "\n",
        "Includes information on advertising data and consumer attention on certain brands."
      ],
      "metadata": {
        "id": "OyFV-TKrZBWs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# choose a couple brands\n",
        "keywords = ['Amazon.com Inc.', 'Apple Inc.', 'Walmart Inc.', 'Target Corp.', 'Costco Wholesale Corp.']\n",
        "\n",
        "# optional (to look at the past five years): pytrends.build_payload(keywords, timeframe = 'today 5-y')\n",
        "pytrends.build_payload(keywords, timeframe = '2020-01-01 2025-01-01')\n",
        "trends_df = pytrends.interest_over_time()"
      ],
      "metadata": {
        "id": "Hi00Ze6FZAst"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# show the trends data\n",
        "trends_df.head()"
      ],
      "metadata": {
        "id": "hOfJGPaMZ8vj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Dataset: US Holiday Calendar**\n",
        "\n",
        "Includes information on US holidays."
      ],
      "metadata": {
        "id": "qiFH_KdFbRO3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# set start abd end dates for the holidays\n",
        "start = pd.Timestamp('2020-01-01')\n",
        "end = pd.Timestamp('2025-12-31')\n",
        "\n",
        "# create DataFrame for US holidays\n",
        "us_holidays = holidays.US(years = range(start.year, end.year + 1))\n",
        "\n",
        "# add only dates in range (some holidays may be out of range)\n",
        "holiday_dates = sorted([pd.Timestamp(d) for d in us_holidays.keys() if start <= pd.Timestamp(d) <= end])\n",
        "holidays_df = pd.DataFrame({'date': holiday_dates, 'holiday_name': [us_holidays[d.date()] for d in holiday_dates]})\n",
        "\n",
        "# show the holidays DataFrame\n",
        "holidays_df.head()"
      ],
      "metadata": {
        "id": "f21q05Imbbm_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Dataset: Merged Data**\n",
        "\n",
        "Includes information on advertising data and consumer attention on certain brands."
      ],
      "metadata": {
        "id": "yohtrWImbYCt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# if 'date' is the index, reset it to make it a column\n",
        "if trends_df.index.name == 'date':\n",
        "    trends_df = trends_df.reset_index()\n",
        "\n",
        "# drop any redundant index columns ('level_0', 'index')\n",
        "if 'level_0' in trends_df.columns:\n",
        "    trends_df = trends_df.drop(columns = ['level_0'])\n",
        "if 'index' in trends_df.columns:\n",
        "    trends_df = trends_df.drop(columns = ['index'])\n",
        "if 'isPartial' in trends_df.columns:\n",
        "    trends_df = trends_df.drop(columns = ['isPartial'])\n",
        "\n",
        "# normalize the trend data\n",
        "trends_df_new = trends_df.melt(id_vars = 'date', var_name = 'brand', value_name = 'google_trend')\n",
        "\n",
        "# converts trends to daily frequency for each brand\n",
        "trends_df_new = (trends_df_new.set_index('date').groupby('brand', group_keys=False).resample('D').ffill().reset_index())\n",
        "\n",
        "# normalize the dates for all the DataFrames\n",
        "ravenpack_df['date'] = pd.to_datetime(ravenpack_df['date']).dt.normalize()\n",
        "trends_df_new['date'] = pd.to_datetime(trends_df_new['date']).dt.normalize()\n",
        "holidays_df['date'] = pd.to_datetime(holidays_df['date']).dt.normalize()\n",
        "\n",
        "# create a merged version of all the data\n",
        "df = trends_df_new.copy()\n",
        "\n",
        "# left merge ravenpack_df\n",
        "df = pd.merge(df, ravenpack_df, on = ['date', 'brand'], how = 'left')\n",
        "\n",
        "# fill NaN sentiment, mentions, buzz with 0 where ravenpack data is missing\n",
        "df['sentiment'] = df['sentiment'].fillna(0)\n",
        "df['mentions'] = df['mentions'].fillna(0)\n",
        "df['buzz'] = df['buzz'].fillna(0)\n",
        "\n",
        "# map the holidays to the nearest trading day\n",
        "trading_calendar = df[['date']].drop_duplicates().sort_values('date')\n",
        "\n",
        "# create an empty list to hold the holidays\n",
        "holiday_trading_days = []\n",
        "\n",
        "# for loop to sort out the holidays\n",
        "for h in holidays_df['date']:\n",
        "    if not trading_calendar.empty:\n",
        "        nearest = trading_calendar.iloc[\n",
        "            (trading_calendar['date'] - h).abs().argsort()[:1]\n",
        "        ]['date'].values[0]\n",
        "        holiday_trading_days.append(nearest)\n",
        "    else:\n",
        "        # fallback if trading_calendar is unexpectedly empty\n",
        "        holiday_trading_days.append(pd.NaT)\n",
        "\n",
        "holidays_adj_df = holidays_df.copy()\n",
        "holidays_adj_df['date'] = holiday_trading_days\n",
        "holidays_adj_df = holidays_adj_df.dropna(subset = ['date']) # drop unmappable holidays\n",
        "\n",
        "# merge the adjusted holiday DataFrame into the DataFrame\n",
        "df = pd.merge(df, holidays_adj_df, on = 'date', how = 'left')\n",
        "\n",
        "# features in the holiday DataFrame to the the binary classification\n",
        "df['is_holiday'] = df['holiday_name'].notna().astype(int)\n",
        "df['holiday_name'] = df['holiday_name'].fillna('None')\n",
        "\n",
        "# feature the holiday give/take days\n",
        "df['holiday_window'] = (df.groupby('brand')['is_holiday'].transform(lambda x: x.rolling(7, center = True, min_periods = 1).max()))\n",
        "\n",
        "# sort the DataFrame\n",
        "df = df.sort_values(['brand', 'date']).reset_index(drop=True)\n",
        "\n",
        "df.info()\n",
        "\n",
        "# show what number is a holiday (1) and what isnt (0)\n",
        "df['is_holiday'].value_counts()"
      ],
      "metadata": {
        "id": "H5ivL0Q0B8cA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# show the merged DataFrame\n",
        "df.head()"
      ],
      "metadata": {
        "id": "c5rds6pphZK3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Nj0gDvdbEA6n"
      },
      "source": [
        "\n",
        "\n",
        "---\n",
        "# **Analysis Questions**\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6L6GWqvUEfLe"
      },
      "source": [
        "**Question 1**: How do holiday advertising affect consumer sentiment levels?"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# copy the adjusted df for analysis\n",
        "df_analysis = df.copy()\n",
        "\n",
        "# compare mean sentiment on holidays vs non-holidays\n",
        "holiday_means = df_analysis.groupby('is_holiday')['sentiment'].mean()\n",
        "\n",
        "print(\"Average sentiment on non-holidays (0) vs holidays (1):\")\n",
        "print(holiday_means)"
      ],
      "metadata": {
        "id": "K-73xiLOoxFx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# plot the relationship between holidays and sentiment\n",
        "sns.boxplot(x = 'is_holiday', y = 'sentiment', data = df_analysis)\n",
        "plt.title(\"Sentiment on Holiday vs Non-Holiday Days\")\n",
        "plt.xlabel(\"Is Holiday\")\n",
        "plt.ylabel(\"Sentiment\")\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "TgUVn4w2qNFs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# regression on the relationship between the holiday and sentiment\n",
        "# single-day holiday effect regression\n",
        "model_simple = smf.ols('sentiment ~ is_holiday + google_trend', data = df_analysis).fit()\n",
        "print(model_simple.summary())"
      ],
      "metadata": {
        "id": "GmoegtEwqQdO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# +/- three-day holiday window effect regression\n",
        "model_window = smf.ols('sentiment ~ holiday_window + google_trend', data = df_analysis).fit()\n",
        "print(model_window.summary())"
      ],
      "metadata": {
        "id": "3pH-UBN5qyn3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EkTjvQPtEjhV"
      },
      "source": [
        "**Question 2**: Is it possible to classify whether a day is “holiday season” or “normal season” using sentiment and engagement data?\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# add rolling averages to capture trends around holidays\n",
        "df['sentiment_roll3'] = df.groupby('brand')['sentiment'].transform(lambda x: x.rolling(3, min_periods = 1).mean())\n",
        "df['buzz_roll3'] = df.groupby('brand')['buzz'].transform(lambda x: x.rolling(3, min_periods = 1).mean())\n",
        "df['mentions_roll3'] = df.groupby('brand')['mentions'].transform(lambda x: x.rolling(3, min_periods = 1).mean())\n",
        "\n",
        "# separate the other factors into X and focus on the holiday y\n",
        "X = df[['sentiment_roll3', 'buzz_roll3', 'mentions_roll3', 'google_trend']]\n",
        "y = df['is_holiday']\n",
        "\n",
        "# split data\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.2, random_state = 42, stratify = y)\n",
        "\n",
        "# scale features\n",
        "scaler = StandardScaler()\n",
        "X_train_scaled = scaler.fit_transform(X_train)\n",
        "X_test_scaled = scaler.transform(X_test)\n",
        "\n",
        "# perform a regression with class weighting\n",
        "clf = LogisticRegression(class_weight = 'balanced', max_iter = 1000, random_state = 42)\n",
        "\n",
        "# fit the model\n",
        "clf.fit(X_train_scaled, y_train)\n",
        "\n",
        "# make predictions to compare\n",
        "y_pred = clf.predict(X_test_scaled)\n",
        "\n",
        "print(\"Confusion Matrix:\")\n",
        "print(confusion_matrix(y_test, y_pred))\n",
        "\n",
        "print(\"\\nClassification Report:\")\n",
        "print(classification_report(y_test, y_pred, zero_division = 0))"
      ],
      "metadata": {
        "id": "BsQfCm3ira_5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fTjDtPdGEmZ3"
      },
      "source": [
        "**Question 3**: Which brands have similar engagement levels during holiday spikes?\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dfc46f35"
      },
      "outputs": [],
      "source": [
        "# find only the days where it is a holiday\n",
        "holiday_df = df[df['is_holiday'] == 1]\n",
        "\n",
        "# filter by engagement\n",
        "brand_engagement = holiday_df.groupby('brand')[['sentiment', 'buzz', 'mentions']].mean()\n",
        "\n",
        "# standardize the values and fit it into the engagement\n",
        "scaler = StandardScaler()\n",
        "X_scaled = scaler.fit_transform(brand_engagement)\n",
        "\n",
        "# do a nearest neighbors model and fit the training model\n",
        "nn = NearestNeighbors(n_neighbors = 5)  # find five similar brands to compare\n",
        "nn.fit(X_scaled)\n",
        "\n",
        "# for each brand, find the five nearest brands to that one\n",
        "distances, neighbors = nn.kneighbors(X_scaled)\n",
        "\n",
        "# get a list of brand names\n",
        "brand_list = brand_engagement.index.tolist()\n",
        "\n",
        "# for each brand, find its info and print it out\n",
        "for i, brand in enumerate(brand_list):\n",
        "    similar_brands = [brand_list[j] for j in neighbors[i] if j != i]\n",
        "    print(f\"{brand} is similar to: {similar_brands}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Question 4**: Can we group brands based on holiday sentiment and buzz patterns? Are some brands “holiday-sensitive” while others remain constant?\n"
      ],
      "metadata": {
        "id": "0J8K0yr2IUKI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# calculates the average sentiment and buzz for each brand at the holiday\n",
        "brand_features = df.groupby(['brand','is_holiday'])[['sentiment','buzz']].mean().unstack()\n",
        "\n",
        "# find how much a brand's sentiment changes during the holiday\n",
        "brand_features['sentiment_change'] = brand_features[('sentiment', 1)] - brand_features[('sentiment', 0)]\n",
        "\n",
        "# find how much a brand's news buzz changes during the holiday\n",
        "brand_features['buzz_change'] = brand_features[('buzz', 1)] - brand_features[('buzz', 0)]\n",
        "\n",
        "# replace any missing values as 0\n",
        "X = brand_features[['sentiment_change','buzz_change']].fillna(0)\n",
        "\n",
        "# perform a KMeans clustering model, three clusers\n",
        "kmeans = KMeans(n_clusters = 3, random_state = 42)\n",
        "# fit the model\n",
        "brand_features['cluster'] = kmeans.fit_predict(X)\n",
        "\n",
        "# plot a cluster graph to show how brands group based on holiday responsiveness\n",
        "plt.scatter(brand_features['sentiment_change'], brand_features['buzz_change'], c = brand_features['cluster'])\n",
        "for i, brand in enumerate(brand_features.index):\n",
        "    plt.text(brand_features['sentiment_change'][i], brand_features['buzz_change'][i], brand)\n",
        "plt.xlabel('Sentiment Change')\n",
        "plt.ylabel('Buzz Change')\n",
        "plt.title('Brand Holiday Sensitivity Clusters')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "GZSJp041IeRV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Question 5**: How do brand sentiment fluctuate 30 days before and after each holiday?"
      ],
      "metadata": {
        "id": "AjXXiVDSIeda"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# create a 60 day winder for each holiday\n",
        "window = 30  # days before and after\n",
        "\n",
        "# filter only holiday days\n",
        "holiday_days = df[df['is_holiday'] == 1]['date'].unique()\n",
        "\n",
        "# create a new DataFrame with sentiment relative to holiday\n",
        "holiday_sentiment_list = []\n",
        "\n",
        "for h in holiday_days:\n",
        "    temp = df[(df['date'] >= h - pd.Timedelta(days=window)) &\n",
        "              (df['date'] <= h + pd.Timedelta(days=window))].copy()\n",
        "    temp['days_from_holiday'] = (temp['date'] - h).dt.days\n",
        "    holiday_sentiment_list.append(temp)\n",
        "\n",
        "# combines all window DataFrames into one large DataFrame\n",
        "holiday_sentiment_df = pd.concat(holiday_sentiment_list)\n",
        "\n",
        "# calculates the average sentiment for each relative day across all holidays\n",
        "sentiment_by_day = holiday_sentiment_df.groupby('days_from_holiday')['sentiment'].mean()\n",
        "\n",
        "# plot the results\n",
        "plt.plot(sentiment_by_day.index, sentiment_by_day.values)\n",
        "plt.axvline(0, color='red', linestyle='--', label='Holiday')\n",
        "plt.xlabel('Days from Holiday')\n",
        "plt.ylabel('Average Sentiment')\n",
        "plt.title('Brand Sentiment 30 Days Before and After Holidays')\n",
        "plt.legend()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "Jh-e20n6IeqK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Question 6**: Using NLP Neural Networks to detect holiday-related topics"
      ],
      "metadata": {
        "id": "-uG9QcOsJLIs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# load a zero-shot classification pipeline\n",
        "classifier = pipeline(\"zero-shot-classification\")\n",
        "\n",
        "# example texts\n",
        "texts = [\n",
        "    \"Cyber Monday deals are great!\",\n",
        "    \"I went to the park today.\",\n",
        "    \"Black Friday sales are amazing!\",\n",
        "    \"Just cooking lunch for my family.\"\n",
        "]\n",
        "\n",
        "# candidate labels\n",
        "labels = [\"holiday\", \"non-holiday\"]\n",
        "\n",
        "# classify each text\n",
        "for text in texts:\n",
        "    result = classifier(text, candidate_labels=labels)\n",
        "    print(f\"'{text}' → {result['labels'][0]} (score: {result['scores'][0]:.2f})\")"
      ],
      "metadata": {
        "id": "J6Ohbd-vwWqF"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}